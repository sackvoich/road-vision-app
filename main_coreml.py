# main_coreml.py
# –î–ª—è –∑–∞–ø—É—Å–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CoreML —Ç—Ä–µ–±—É–µ—Ç—Å—è macOS –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:
# pip install coremltools
#
# –ù–∞ –¥—Ä—É–≥–∏—Ö –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞—Ö –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –∏–º–∏—Ç–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã CoreML –º–æ–¥–µ–ª–µ–π

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse
import cv2
import numpy as np
import base64
import asyncio
import json
import logging
from PIL import Image

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å coremltools, —Å fallback –Ω–∞ –∏–º–∏—Ç–∞—Ü–∏—é
try:
    import coremltools as ct
    COREML_AVAILABLE = True
except ImportError:
    logging.warning("coremltools not available. Using mock implementation.")
    COREML_AVAILABLE = False
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏–º–∏—Ç–∞—Ü–∏–∏ –º–æ–¥—É–ª—è coremltools –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –≤–Ω–µ macOS
    class MockMLModel:
        def __init__(self, path):
            self.path = path
            logging.info(f"Mock loading model from {path}")
        
        def predict(self, input_dict):
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            logging.info("Mock prediction")
            # –§–∏–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–µ—Ç–µ–∫—Ü–∏–∏
            if 'traffic_sign' in self.path:
                return {
                    'confidence': np.array([0.9, 0.7]),
                    'coordinates': np.array([[0.3, 0.4, 0.1, 0.1], [0.7, 0.6, 0.05, 0.05]])
                }
            # –§–∏–∫—Ç–∏–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
            else:
                return {
                    'output': np.random.rand(1, 640, 640, 1).astype(np.float32)
                }
        
        def get_spec(self):
            class MockSpec:
                class MockDescription:
                    def __init__(self):
                        self.input = "Mock input description"
                def __init__(self):
                    self.description = self.MockDescription()
            return MockSpec()
    
    class ct:
        class models:
            MLModel = MockMLModel

app = FastAPI(title="CoreML Video Processor")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π CoreML
try:
    traffic_signs_model = ct.models.MLModel('./models/traffic_signs_detection_model.mlpackage')
    zebra_model = ct.models.MLModel('./models/zebra_segmentation_model.mlpackage')
    
    # –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    traffic_signs_spec = traffic_signs_model.get_spec()
    zebra_spec = zebra_model.get_spec()
    
    logging.info(f"Traffic signs model loaded. Input description: {traffic_signs_spec.description.input}")
    logging.info(f"Zebra segmentation model loaded. Input description: {zebra_spec.description.input}")
    
except Exception as e:
    logging.error(f"Failed to load CoreML models: {e}")
    raise

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è FPS
last_processed_time = 0
# –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ–±—Ä–∞–±–æ—Ç–∫–∞–º–∏ –∫–∞–¥—Ä–æ–≤ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö)
# –î–ª—è 5 FPS —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ 0.2, –¥–ª—è 10 FPS - 0.1, –¥–ª—è 2 FPS - 0.5
min_interval = 0.05  # 5 FPS –º–∞–∫—Å–∏–º—É–º (–∏–∑–º–µ–Ω—è–π –ø–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)

class ConnectionManager:
    def __init__(self):
        self.active_connections = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)

    async def send_json(self, data: dict, websocket: WebSocket):
        await websocket.send_json(data)

manager = ConnectionManager()

def preprocess_image_for_coreml(frame, target_size=(640, 640)):
    """
    –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–¥–∞—á–∏ –≤ CoreML –º–æ–¥–µ–ª—å
    
    Args:
        frame: –í—Ö–æ–¥–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (numpy array)
        target_size: –¶–µ–ª–µ–≤–æ–π —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (—à–∏—Ä–∏–Ω–∞, –≤—ã—Å–æ—Ç–∞)
    
    Returns:
        –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ PIL Image
    """
    # –ò–∑–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–æ —Ä–∞–∑–º–µ—Ä–∞, –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –º–æ–¥–µ–ª—å—é
    resized_frame = cv2.resize(frame, target_size)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR (OpenCV) –≤ RGB (PIL)
    rgb_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)
    
    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º numpy array –≤ PIL Image
    pil_image = Image.fromarray(rgb_frame)
    
    return pil_image

def process_frame_with_traffic_signs_model(frame_data: str) -> dict:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –º–æ–¥–µ–ª—å—é –¥–µ—Ç–µ–∫—Ü–∏–∏ –∑–Ω–∞–∫–æ–≤"""
    try:
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 –≤ numpy array
        nparr = np.frombuffer(base64.b64decode(frame_data), np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if frame is None:
            return {"error": "Failed to decode image"}
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è CoreML –º–æ–¥–µ–ª–∏
        input_image = preprocess_image_for_coreml(frame)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
        input_dict = {"image": input_image}
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –æ–±—ä–µ–∫—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é CoreML –º–æ–¥–µ–ª–∏
        results = traffic_signs_model.predict(input_dict)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        detections = []
        
        # –î–ª—è –º–æ–¥–µ–ª–∏ YOLO –≤ CoreML –≤—ã—Ö–æ–¥–Ω–æ–π —Ñ–æ—Ä–º–∞—Ç –æ–±—ã—á–Ω–æ:
        # [batch, num_classes + 5, num_predictions] –≥–¥–µ 5 —ç—Ç–æ (x, y, w, h, confidence)
        # –í –Ω–∞—à–µ–º —Å–ª—É—á–∞–µ: [1, 159, 8400]
        if 'var_1932' in results:
            output = results['var_1932']
            if isinstance(output, np.ndarray) and len(output.shape) == 3:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
                predictions = output[0]  # –£–±–∏—Ä–∞–µ–º batch dimension
                
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                threshold = 0.5
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                # predictions.shape = [159, 8400]
                # –ü–µ—Ä–≤—ã–µ 4 —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –∫–∞–∂–¥–æ–º —Å—Ç–æ–ª–±—Ü–µ - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (x, y, w, h)
                # –ü—è—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∞
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
                
                for i in range(predictions.shape[1]):
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞
                    obj_confidence = predictions[4, i]
                    
                    if obj_confidence > threshold:
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                        x, y, w, h = predictions[0:4, i]
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
                        class_scores = predictions[5:, i]
                        
                        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                        class_id = np.argmax(class_scores)
                        class_confidence = class_scores[class_id]
                        
                        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∞ * —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞
                        confidence = obj_confidence * class_confidence
                        
                        if confidence > threshold:
                            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ (center_x, center_y, width, height) –≤ (x1, y1, x2, y2)
                            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (0-1), –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–∏–∫—Å–µ–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            x1 = max(0, (x - w/2) * frame.shape[1])
                            y1 = max(0, (y - h/2) * frame.shape[0])
                            x2 = min(frame.shape[1], (x + w/2) * frame.shape[1])
                            y2 = min(frame.shape[0], (y + h/2) * frame.shape[0])
                            
                            detections.append({
                                'class': f'class_{class_id}',  # –í—Ä–µ–º–µ–Ω–Ω–æ–µ –∏–º—è –∫–ª–∞—Å—Å–∞
                                'class_id': int(class_id),
                                'confidence': float(confidence),
                                'bbox': [float(x1), float(y1), float(x2), float(y2)]
                            })
        
        return {
            'detections': detections,
            'timestamp': asyncio.get_event_loop().time(),
            'objects_count': len(detections)
        }
    
    except Exception as e:
        logging.error(f"Error processing frame with traffic signs model: {e}")
        return {'detections': [], 'error': str(e)}

def process_frame_with_zebra_model(frame_data: str) -> dict:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –º–æ–¥–µ–ª—å—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–µ–±—Ä"""
    try:
        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º base64 –≤ numpy array
        nparr = np.frombuffer(base64.b64decode(frame_data), np.uint8)
        frame = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if frame is None:
            return {"error": "Failed to decode image"}
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è CoreML –º–æ–¥–µ–ª–∏
        input_image = preprocess_image_for_coreml(frame)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–æ–¥–µ–ª–∏
        input_dict = {"image": input_image}
        
        # –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è —Å –ø–æ–º–æ—â—å—é CoreML –º–æ–¥–µ–ª–∏
        results = zebra_model.predict(input_dict)
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        segments = []
        
        # –î–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –¥–≤–∞ –≤—ã—Ö–æ–¥–∞:
        # 1. 'var_1368': [1, 37, 8400] - –≤–µ—Ä–æ—è—Ç–Ω–æ, –¥–µ—Ç–µ–∫—Ü–∏–∏ (–ø–æ—Ö–æ–∂–µ –Ω–∞ YOLO —Ñ–æ—Ä–º–∞—Ç)
        # 2. 'p': [1, 32, 160, 160] - –≤–µ—Ä–æ—è—Ç–Ω–æ, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Å–∫–∞
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–π –º–∞—Å–∫–∏
        if 'p' in results:
            output = results['p']
            if isinstance(output, np.ndarray) and len(output.shape) == 4:
                # –§–æ—Ä–º–∞: [batch, channels, height, width]
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å–∫—É (–ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –∫–∞–Ω–∞–ª 0 —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–µ–±—Ä–∞—Ö)
                mask = output[0, 0, :, :]  # –§–æ—Ä–º–∞: [160, 160]
                
                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–∏–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏ –º–∞—Å–∫–∏
                threshold = 0.5
                binary_mask = (mask > threshold).astype(np.uint8)
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–∏–∫—Å–µ–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                if np.sum(binary_mask) > 0:
                    segments.append({
                        'type': 'zebra_crossing',
                        'confidence': float(np.mean(mask)),  # –°—Ä–µ–¥–Ω—è—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
                        'mask_shape': binary_mask.shape
                    })
        
        # –ï—Å–ª–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Å–∫–∞ –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –ø—Ä–æ–±—É–µ–º –¥–µ—Ç–µ–∫—Ü–∏–∏
        if not segments and 'var_1368' in results:
            output = results['var_1368']
            if isinstance(output, np.ndarray) and len(output.shape) == 3:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–µ—Ç–µ–∫—Ü–∏–∏
                predictions = output[0]  # –£–±–∏—Ä–∞–µ–º batch dimension
                
                # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä–æ–≥–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
                threshold = 0.5
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                # predictions.shape = [37, 8400]
                # –ü–µ—Ä–≤—ã–µ 4 —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –∫–∞–∂–¥–æ–º —Å—Ç–æ–ª–±—Ü–µ - –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (x, y, w, h)
                # –ü—è—Ç—ã–π —ç–ª–µ–º–µ–Ω—Ç - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∞
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ - —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
                
                for i in range(predictions.shape[1]):
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –æ–±—ä–µ–∫—Ç–∞
                    obj_confidence = predictions[4, i]
                    
                    if obj_confidence > threshold:
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
                        x, y, w, h = predictions[0:4, i]
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤
                        class_scores = predictions[5:, i]
                        
                        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é
                        class_id = np.argmax(class_scores)
                        class_confidence = class_scores[class_id]
                        
                        # –û–±—â–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å = —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –æ–±—ä–µ–∫—Ç–∞ * —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞
                        confidence = obj_confidence * class_confidence
                        
                        if confidence > threshold:
                            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ (center_x, center_y, width, height) –≤ (x1, y1, x2, y2)
                            # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã (0-1), –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –ø–∏–∫—Å–µ–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                            x1 = max(0, (x - w/2) * frame.shape[1])
                            y1 = max(0, (y - h/2) * frame.shape[0])
                            x2 = min(frame.shape[1], (x + w/2) * frame.shape[1])
                            y2 = min(frame.shape[0], (y + h/2) * frame.shape[0])
                            
                            segments.append({
                                'type': 'zebra_crossing',
                                'confidence': float(confidence),
                                'bbox': [float(x1), float(y1), float(x2), float(y2)]
                            })
        
        return {
            'segments': segments,
            'timestamp': asyncio.get_event_loop().time(),
            'segments_count': len(segments)
        }
    
    except Exception as e:
        logging.error(f"Error processing frame with zebra model: {e}")
        return {'segments': [], 'error': str(e)}

@app.websocket("/ws/video/traffic_signs")
async def websocket_traffic_signs_endpoint(websocket: WebSocket):
    """WebSocket —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–∞ –º–æ–¥–µ–ª—å—é –¥–µ—Ç–µ–∫—Ü–∏–∏ –∑–Ω–∞–∫–æ–≤"""
    await manager.connect(websocket)
    global last_processed_time
    
    try:
        while True:
            # –ü–æ–ª—É—á–∞–µ–º –∫–∞–¥—Ä –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞
            data = await websocket.receive_text()
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ FPS - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä—ã –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ
            current_time = asyncio.get_event_loop().time()
            if current_time - last_processed_time < min_interval:
                # –°–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä
                await websocket.send_json({
                    "status": "skipped",
                    "message": "Frame rate limited"
                })
                continue
            
            last_processed_time = current_time
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –º–æ–¥–µ–ª—å—é –¥–µ—Ç–µ–∫—Ü–∏–∏ –∑–Ω–∞–∫–æ–≤
            result = process_frame_with_traffic_signs_model(data)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ
            await manager.send_json(result, websocket)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
        print("Client disconnected from traffic signs endpoint")
    except Exception as e:
        logging.error(f"WebSocket error in traffic signs endpoint: {e}")
        manager.disconnect(websocket)

@app.websocket("/ws/video/zebra")
async def websocket_zebra_endpoint(websocket: WebSocket):
    """WebSocket —ç–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–∏–¥–µ–æ –ø–æ—Ç–æ–∫–∞ –º–æ–¥–µ–ª—å—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–µ–±—Ä"""
    await manager.connect(websocket)
    global last_processed_time
    
    try:
        while True:
            # –ü–æ–ª—É—á–∞–µ–º –∫–∞–¥—Ä –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞
            data = await websocket.receive_text()
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ FPS - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä—ã –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º —á–∞—Å—Ç–æ
            current_time = asyncio.get_event_loop().time()
            if current_time - last_processed_time < min_interval:
                # –°–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–¥—Ä
                await websocket.send_json({
                    "status": "skipped",
                    "message": "Frame rate limited"
                })
                continue
            
            last_processed_time = current_time
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–¥—Ä–∞ –º–æ–¥–µ–ª—å—é —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–µ–±—Ä
            result = process_frame_with_zebra_model(data)
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ
            await manager.send_json(result, websocket)
            
    except WebSocketDisconnect:
        manager.disconnect(websocket)
        print("Client disconnected from zebra endpoint")
    except Exception as e:
        logging.error(f"WebSocket error in zebra endpoint: {e}")
        manager.disconnect(websocket)

# –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Ñ–∞–π–ª –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞
@app.get("/")
async def get_frontend():
    return HTMLResponse("""
    <!DOCTYPE html>
    <html>
    <head>
        <title>CoreML Video Stream Processor</title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <style>
            body { font-family: Arial, sans-serif; margin: 20px; }
            .container { max-width: 800px; margin: 0 auto; }
            .video-container { display: flex; gap: 20px; margin-bottom: 20px; }
            video, canvas { width: 300px; height: 225px; border: 2px solid #ccc; }
            button { padding: 10px 20px; margin: 5px; font-size: 16px; }
            .controls { margin-bottom: 20px; }
            .results { background: #f5f5f5; padding: 15px; border-radius: 5px; }
            .model-selector { margin: 15px 0; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üìπ CoreML Video Stream Processor</h1>
            
            <div class="model-selector">
                <label>–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å:</label>
                <select id="modelSelector">
                    <option value="traffic_signs">–î–µ—Ç–µ–∫—Ü–∏—è –¥–æ—Ä–æ–∂–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤</option>
                    <option value="zebra">–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –∑–µ–±—Ä</option>
                </select>
            </div>
            
            <div class="controls">
                <button id="startBtn">‚ñ∂Ô∏è Start Streaming</button>
                <button id="stopBtn" disabled>‚èπÔ∏è Stop Streaming</button>
                <span id="status">Status: Ready</span>
            </div>

            <div class="video-container">
                <div>
                    <h3>Live Camera</h3>
                    <video id="video" autoplay muted playsinline></video>
                </div>
                <div>
                    <h3>Processed</h3>
                    <canvas id="canvas"></canvas>
                </div>
            </div>

            <div class="results">
                <h3>Processing Results:</h3>
                <pre id="results"></pre>
            </div>
        </div>

        <script>
        class VideoProcessor {
            constructor() {
                this.video = document.getElementById('video');
                this.canvas = document.getElementById('canvas');
                this.ctx = this.canvas.getContext('2d');
                this.resultsElement = document.getElementById('results');
                this.statusElement = document.getElementById('status');
                this.ws = null;
                this.isStreaming = false;
                // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º FPS –Ω–∞ –∫–ª–∏–µ–Ω—Ç–µ —Ç–æ–∂–µ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω —Å —Å–µ—Ä–≤–µ—Ä–æ–º)
                // –î–ª—è 5 FPS —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ 5, –¥–ª—è 10 FPS - 10, –¥–ª—è 2 FPS - 2
                this.fps = 5; // –ò–∑–º–µ–Ω—è–π—Ç–µ —ç—Ç–æ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —Å–µ—Ä–≤–µ—Ä–∞
                
                this.setupEventListeners();
                this.initializeCamera();
            }

            setupEventListeners() {
                document.getElementById('startBtn').addEventListener('click', () => this.startStreaming());
                document.getElementById('stopBtn').addEventListener('click', () => this.stopStreaming());
            }

            async initializeCamera() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { 
                            width: { ideal: 640 },
                            height: { ideal: 480 },
                            facingMode: 'environment' // –ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–Ω—é—é –∫–∞–º–µ—Ä—É
                        },
                        audio: false
                    });
                    this.video.srcObject = stream;
                    this.updateStatus('Camera ready');
                } catch (error) {
                    this.updateStatus('Camera error: ' + error.message);
                    console.error('Camera error:', error);
                }
            }

            updateStatus(message) {
                this.statusElement.textContent = 'Status: ' + message;
            }

            startStreaming() {
                if (this.isStreaming) return;

                // –ü–æ–ª—É—á–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏
                const modelType = document.getElementById('modelSelector').value;
                let wsUrl;
                
                if (modelType === 'traffic_signs') {
                    wsUrl = `ws://${window.location.host}/ws/video/traffic_signs`;
                } else {
                    wsUrl = `ws://${window.location.host}/ws/video/zebra`;
                }

                this.ws = new WebSocket(wsUrl);
                
                this.ws.onopen = () => {
                    this.isStreaming = true;
                    this.updateStatus('Streaming with ' + modelType + ' model...');
                    document.getElementById('startBtn').disabled = true;
                    document.getElementById('stopBtn').disabled = false;
                    this.sendFrames();
                };

                this.ws.onmessage = (event) => {
                    const data = JSON.parse(event.data);
                    this.displayResults(data);
                    this.drawResults(data);
                };

                this.ws.onerror = (error) => {
                    this.updateStatus('WebSocket error');
                    console.error('WebSocket error:', error);
                };

                this.ws.onclose = () => {
                    this.isStreaming = false;
                    this.updateStatus('Disconnected');
                    document.getElementById('startBtn').disabled = false;
                    document.getElementById('stopBtn').disabled = true;
                };
            }

            stopStreaming() {
                if (this.ws) {
                    this.ws.close();
                }
                this.isStreaming = false;
                this.updateStatus('Stopped');
                document.getElementById('startBtn').disabled = false;
                document.getElementById('stopBtn').disabled = true;
            }

            sendFrames() {
                if (!this.isStreaming) return;

                // –†–∏—Å—É–µ–º —Ç–µ–∫—É—â–∏–π –∫–∞–¥—Ä –Ω–∞ canvas
                this.ctx.drawImage(this.video, 0, 0, this.canvas.width, this.canvas.height);
                
                // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
                const imageData = this.canvas.toDataURL('image/jpeg', 0.7);
                const base64Data = imageData.split(',')[1];

                // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ —Å–µ—Ä–≤–µ—Ä
                if (this.ws && this.ws.readyState === WebSocket.OPEN) {
                    this.ws.send(base64Data);
                }

                // –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º FPS
                setTimeout(() => this.sendFrames(), 1000 / this.fps);
            }

            displayResults(data) {
                this.resultsElement.textContent = JSON.stringify(data, null, 2);
            }

            drawResults(data) {
                if (data.detections) {
                    this.drawDetections(data.detections);
                } else if (data.segments) {
                    this.drawSegments(data.segments);
                }
            }

            drawDetections(detections) {
                if (!detections) return;

                // –û—á–∏—â–∞–µ–º canvas
                this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
                
                // –†–∏—Å—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                this.ctx.drawImage(this.video, 0, 0, this.canvas.width, this.canvas.height);
                
                // –†–∏—Å—É–µ–º bounding boxes
                detections.forEach(det => {
                    const [x1, y1, x2, y2] = det.bbox;
                    
                    // –†–∏—Å—É–µ–º –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫
                    this.ctx.strokeStyle = '#00ff00';
                    this.ctx.lineWidth = 2;
                    this.ctx.strokeRect(x1, y1, x2 - x1, y2 - y1);
                    
                    // –ü–æ–¥–ø–∏—Å—å
                    this.ctx.fillStyle = '#00ff00';
                    this.ctx.font = '12px Arial';
                    this.ctx.fillText(
                        `${det.class} (${(det.confidence * 100).toFixed(1)}%)`,
                        x1,
                        y1 - 5
                    );
                });
            }

            drawSegments(segments) {
                if (!segments) return;

                // –û—á–∏—â–∞–µ–º canvas
                this.ctx.clearRect(0, 0, this.canvas.width, this.canvas.height);
                
                // –†–∏—Å—É–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                this.ctx.drawImage(this.video, 0, 0, this.canvas.width, this.canvas.height);
                
                // –†–∏—Å—É–µ–º –º–∞—Å–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ)
                segments.forEach(segment => {
                    // –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –Ω–∞–ª–æ–∂–µ–Ω–∏–µ –º–∞—Å–∫–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
                    this.ctx.fillStyle = 'rgba(255, 0, 0, 0.3)';
                    this.ctx.fillRect(50, 50, 200, 100); // –ü—Ä–∏–º–µ—Ä –æ–±–ª–∞—Å—Ç–∏ —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ –∑–µ–±—Ä—ã
                    
                    // –ü–æ–¥–ø–∏—Å—å
                    this.ctx.fillStyle = '#ff0000';
                    this.ctx.font = '12px Arial';
                    this.ctx.fillText(
                        `Zebra Crossing`,
                        50,
                        45
                    );
                });
            }
        }

        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        document.addEventListener('DOMContentLoaded', () => {
            new VideoProcessor();
        });
        </script>
    </body>
    </html>
    """)

# –ú–æ–Ω—Ç–∏—Ä—É–µ–º —Å—Ç–∞—Ç–∏–∫—É
app.mount("/static", StaticFiles(directory="static"), name="static")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")